{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ENVIRONMENT SETUP (Kaggle)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Clears workspace, clones LabelBench, installs dependencies\n# ============================================================\n\n!rm -rf /kaggle/working/*\n!git clone https://github.com/EfficientTraining/LabelBench.git\n!pip install -r /kaggle/working/LabelBench/requirements.txt\n%cd /kaggle/working/LabelBench\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T04:54:55.590073Z","iopub.execute_input":"2026-02-05T04:54:55.590391Z","iopub.status.idle":"2026-02-05T04:54:55.708560Z","shell.execute_reply.started":"2026-02-05T04:54:55.590363Z","shell.execute_reply":"2026-02-05T04:54:55.707954Z"}},"outputs":[{"name":"stdout","text":"\u001b[0m\u001b[01;34mconfigs\u001b[0m/        \u001b[01;34mLabelBench\u001b[0m/  mp_eval_launcher.py  README.md\n\u001b[01;34mdocs\u001b[0m/           LICENSE      mp_launcher.py       requirements.txt\nexample_run.sh  main.py      point_evaluation.py  \u001b[01;34mresults\u001b[0m/\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## DATASET SETUP: CIFAR-10 STREAM FOR OPEN-WORLD LEARNING","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torchvision import datasets as tv_datasets, transforms\nfrom torch.utils.data import Dataset\nfrom LabelBench.skeleton.dataset_skeleton import register_dataset, LabelType, TransformDataset\nimport numpy as np\n\nNUM_TASKS = 20   # Total number of incremental tasks\n\n\n# ------------------------------------------------------------\n# Dataset wrapper to expose only a subset of CIFAR-10 indices\n# ------------------------------------------------------------\nclass CIFARStream(Dataset):\n    def __init__(self, base_ds, indices):\n        self.base_ds = base_ds      # Full CIFAR-10 dataset\n        self.indices = indices      # Indices assigned to this task\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        # Fetch the actual sample using stored indices\n        x, y = self.base_ds[self.indices[idx]]\n        return x, y\n\n\n# ------------------------------------------------------------\n# One-hot encoding helper (required by LabelBench)\n# ------------------------------------------------------------\ndef one_hot(y, n=10):\n    return F.one_hot(torch.tensor(y), num_classes=n).float()\n\n\n# ------------------------------------------------------------\n# Base dataset registry (intentionally disabled)\n# ------------------------------------------------------------\n@register_dataset(\"splitcifar10\", LabelType.MULTI_CLASS)\ndef get_splitcifar10(_):\n    # Prevent accidental usage of the base dataset\n    raise RuntimeError(\"Use splitcifar10_<id>\")\n\n\n# ------------------------------------------------------------\n# Build task stream ONCE (randomized split of CIFAR-10)\n# ------------------------------------------------------------\nbase_train_global = tv_datasets.CIFAR10(root=\"./data\", train=True, download=True)\nall_idx = np.arange(len(base_train_global))\nnp.random.shuffle(all_idx)\nstream_splits = np.array_split(all_idx, NUM_TASKS)\n\n\n# ------------------------------------------------------------\n# Register each task dynamically\n# ------------------------------------------------------------\nfor split_id in range(NUM_TASKS):\n\n    @register_dataset(f\"splitcifar10_{split_id}\", LabelType.MULTI_CLASS)\n    def _make_split(data_dir, split_id=split_id):\n\n        tf = transforms.Compose([transforms.ToTensor()])\n\n        base_train = tv_datasets.CIFAR10(root=data_dir, train=True, download=True)\n        base_test  = tv_datasets.CIFAR10(root=data_dir, train=False, download=True)\n\n        # ----------------------------------------------------\n        # TASK 0: Base session (only classes 0 and 1)\n        # Fully supervised\n        # ----------------------------------------------------\n        if split_id == 0:\n            indices = [i for i,(x,y) in enumerate(base_train) if y in [0,1]]\n        else:\n            # ------------------------------------------------\n            # TASKS 1â€“19: Unlabeled streaming data\n            # ------------------------------------------------\n            indices = stream_splits[split_id]\n\n        train_ds = CIFARStream(base_train, indices)\n\n        # Wrap dataset with transforms + one-hot labels\n        train_ds = TransformDataset(\n            train_ds,\n            transform=tf,\n            target_transform=lambda y: one_hot(y,10)\n        )\n\n        test_ds = TransformDataset(\n            base_test,\n            transform=tf,\n            target_transform=lambda y: one_hot(y,10)\n        )\n\n        return train_ds, test_ds, test_ds, None, None, None, 10, [str(i) for i in range(10)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T04:54:59.523636Z","iopub.execute_input":"2026-02-05T04:54:59.524056Z","iopub.status.idle":"2026-02-05T04:55:08.448101Z","shell.execute_reply.started":"2026-02-05T04:54:59.524026Z","shell.execute_reply":"2026-02-05T04:55:08.447507Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:01<00:00, 101MB/s]  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Model Training with Novelty Detection and Clustering","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# MODEL DEFINITION (ResNet18 + Embedding Head)\n# ============================================================\n\nimport torch.nn as nn\nfrom collections import defaultdict, Counter\nfrom torch.utils.data import DataLoader\nimport hdbscan\nfrom torchvision.models import resnet18\nfrom LabelBench.skeleton.dataset_skeleton import datasets as DATASET_REGISTRY\n\n\nclass CNN(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        # Pretrained ResNet-18 backbone\n        base = resnet18(pretrained=True)\n\n        # Remove classification head â†’ keep feature extractor\n        self.encoder = nn.Sequential(*list(base.children())[:-1])\n\n        # Project features to a compact embedding space\n        self.embed = nn.Linear(512, 128)\n\n        # Classification head (dynamically expanded later)\n        self.classifier = nn.Linear(128, num_classes)\n\n    def expand_head(self, new_classes):\n        # Save old classifier weights\n        old_w = self.classifier.weight.data.clone()\n        old_b = self.classifier.bias.data.clone()\n        old_n = old_w.shape[0]\n\n        # Create a larger classifier\n        self.classifier = nn.Linear(128, new_classes)\n\n        # Copy old weights into the new head\n        self.classifier.weight.data[:old_n] = old_w\n        self.classifier.bias.data[:old_n] = old_b\n\n    def forward(self, x):\n        # Extract CNN features\n        z = self.encoder(x).squeeze()\n\n        # Project to embedding\n        z = self.embed(z)\n\n        # Normalize embeddings (important for cosine geometry)\n        z = F.normalize(z, dim=1)\n\n        # Classification logits\n        logits = self.classifier(z)\n\n        return logits, z\n\n\n# ============================================================\n# MEMORY BUFFER (CLASS PROTOTYPES)\n# ============================================================\n\nclass MemoryBuffer:\n    def __init__(self, max_per_class=20):\n        # Stores embeddings per class\n        self.data = defaultdict(list)\n        self.max_per_class = max_per_class\n\n    def add_batch(self, Z, y):\n        # Add embeddings to memory\n        for z in Z:\n            self.data[int(y)].append(z.detach().cpu())\n\n        # Reduce memory to fixed size\n        self._reduce_class(y)\n\n    def _reduce_class(self, y):\n        Z = self.data[int(y)]\n\n        # Do nothing if under capacity\n        if len(Z) <= self.max_per_class:\n            return\n\n        Z = torch.stack(Z)\n\n        # Compute class centroid (direction)\n        mu = F.normalize(Z.mean(0), dim=0)\n\n        # Cosine distance to centroid\n        d = 1 - torch.matmul(Z, mu)\n\n        # Keep closest samples to centroid\n        idx = torch.argsort(d)[:self.max_per_class]\n\n        self.data[int(y)] = [Z[i] for i in idx]\n\n    def get(self):\n        return self.data\n\n\n# ============================================================\n# NOVELTY DETECTOR (HYPERSPHERE PER CLASS)\n# ============================================================\n\nclass HypersphereNovelty:\n    def __init__(self, q=0.95):\n        self.q = q    # Quantile used to define class radius\n        self.mu = {}  # Class centroids\n        self.r = {}   # Class radii\n\n    def update(self, memory):\n        # Recompute centroid and radius for each known class\n        self.mu, self.r = {}, {}\n\n        for k, Z in memory.items():\n            Z = torch.stack(Z)\n            mu = F.normalize(Z.mean(0), dim=0)\n\n            # Distance of samples from centroid\n            d = 1 - torch.matmul(Z, mu)\n\n            # Radius enclosing q% of samples\n            r = torch.quantile(d, self.q)\n\n            self.mu[k] = mu\n            self.r[k] = r\n\n    def score(self, z):\n        # Novelty score = how far sample is from nearest known class\n        scores = []\n\n        for k in self.mu:\n            d = 1 - torch.dot(z.cpu(), self.mu[k])\n            scores.append(d - self.r[k])\n\n        return min(scores)\n\n\n# ============================================================\n# TRAINING FUNCTIONS\n# ============================================================\n\ndef train_supervised(model, loader, device, epochs=15):\n    # Supervised training for base task (Task 0)\n    opt = torch.optim.Adam(model.parameters(), 1e-4)\n    model.train()\n\n    for _ in range(epochs):\n        for x, y in loader:\n            x = x.to(device)\n            y = y.argmax(1).to(device)\n\n            logits, _ = model(x)\n            loss = F.cross_entropy(logits, y)\n\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n\ndef finetune(model, memory, Z_new, new_label, device, epochs=3):\n    # Finetunes classifier using memory + new class samples\n    X, Y = [], []\n\n    for cls, Zs in memory.items():\n        for z in Zs:\n            X.append(z)\n            Y.append(cls)\n\n    for z in Z_new:\n        X.append(z.cpu())\n        Y.append(new_label)\n\n    X = torch.stack(X).detach().to(device)\n    Y = torch.tensor(Y).to(device)\n\n    opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n    model.train()\n\n    for _ in range(epochs):\n        logits = model.classifier(X)\n        loss = F.cross_entropy(logits, Y)\n\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n\n# ============================================================\n# MAIN OPEN-WORLD LEARNING LOOP\n# ============================================================\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nTASKS = [f\"splitcifar10_{i}\" for i in range(NUM_TASKS)]\n\nmodel = CNN(num_classes=2).to(device)\nmemory = MemoryBuffer(max_per_class=20)\ndetector = HypersphereNovelty(q=0.95)\n\nnovelty_buffer = []   # Stores unresolved novel samples\nknown_classes = 2     # Starts with classes {0,1}\n\n# Tuned thresholds\nCOH_THR = 0.35\nSEP_THR = 0.1\nPURITY_THR = 0.35\nNOVELTY_THR = 0.3\n\nP = 4\nMAX_NOVELTY_BUFFER = 300\nTOPK_NOVELTY = 120\n\n\nfor t, task in enumerate(TASKS):\n\n    print(f\"\\n================ TASK {t} ================\")\n\n    _, dataset_fn = DATASET_REGISTRY[task]\n    train_ds, _, _, _, _, _, _, _ = dataset_fn(\"./data\")\n    loader = DataLoader(train_ds, batch_size=64, shuffle=False)\n\n    # --------------------------------------------------------\n    # TASK 0: Base supervised training\n    # --------------------------------------------------------\n    if t == 0:\n        train_supervised(model, loader, device)\n\n        with torch.no_grad():\n            for x, y in loader:\n                x = x.to(device)\n                y = y.argmax(1)\n\n                _, z = model(x)\n\n                for cls in torch.unique(y):\n                    idx = (y == cls).nonzero().squeeze()\n                    memory.add_batch(z[idx], cls.item())\n\n        detector.update(memory.get())\n        print(\"âœ… [INFO] Task0 training complete\")\n        continue\n\n    # --------------------------------------------------------\n    # STAGE I: Novelty routing\n    # --------------------------------------------------------\n    model.eval()\n    novelty_candidates = []\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.argmax(1)\n\n            _, z = model(x)\n\n            for i in range(len(z)):\n                s = detector.score(z[i])\n\n                if s > NOVELTY_THR:\n                    novelty_candidates.append((s, z[i].cpu(), y[i].item()))\n\n    novelty_candidates.sort(key=lambda x: x[0], reverse=True)\n    novelty_candidates = novelty_candidates[:TOPK_NOVELTY]\n\n    novelty_buffer.extend([(z, y) for _, z, y in novelty_candidates])\n\n    if len(novelty_buffer) > MAX_NOVELTY_BUFFER:\n        novelty_buffer = novelty_buffer[-MAX_NOVELTY_BUFFER:]\n\n    print(f\"[DEBUG] Task {t} novelty buffer size = {len(novelty_buffer)}\")\n\n    # --------------------------------------------------------\n    # STAGE II: Clustering (every P tasks)\n    # --------------------------------------------------------\n    if (t % P) != 0:\n        print(\"[DEBUG] Skipping Stage II\")\n        continue\n\n    print(\"[DEBUG] Entering Stage II clustering\")\n    print(\"Novelty buffer label distribution:\", Counter([y for _, y in novelty_buffer]))\n\n    Z = np.stack([z.numpy() for z, _ in novelty_buffer])\n\n    labels = hdbscan.HDBSCAN(\n        metric='euclidean',\n        min_cluster_size=5,\n        min_samples=3\n    ).fit_predict(Z)\n\n    print(f\"[DEBUG] Clusters found: {set(labels)}\")\n\n    new_buffer = []\n\n    for cid in set(labels):\n        idxs = np.where(labels == cid)[0]\n\n        # ------------------------------\n        # Noise cluster â†’ keep samples\n        # ------------------------------\n        if cid == -1:\n            print(\"Noise cluster kept\")\n            for i in idxs:\n                new_buffer.append(novelty_buffer[i])\n            continue\n\n        cluster = [novelty_buffer[i] for i in idxs]\n        Zc = torch.stack([z for z, _ in cluster])\n        true_labels = [y for _, y in cluster]\n\n        mu_c = F.normalize(Zc.mean(0), dim=0)\n        d = 1 - torch.matmul(Zc, mu_c)\n        Scoh = torch.quantile(d, 0.9)\n\n        sep = min([(1 - torch.dot(mu_c, detector.mu[k])) - detector.r[k] for k in detector.mu])\n\n        counter = Counter(true_labels)\n        semantic_label, count = counter.most_common(1)[0]\n        purity = count / len(true_labels)\n\n        print(f\"ðŸ“Š [CLUSTER {cid}] size={len(cluster)} | Scoh={Scoh:.3f} | Sep={sep:.3f} | semantic_label={semantic_label} | purity={purity:.2f}\")\n\n        # ------------------------------\n        # Reject but KEEP samples\n        # ------------------------------\n        if semantic_label < known_classes or purity < PURITY_THR:\n            print(\"Blocked (kept)\")\n            for i in idxs:\n                new_buffer.append(novelty_buffer[i])\n            continue\n\n        # ------------------------------\n        # Promote new class\n        # ------------------------------\n        if Scoh <= COH_THR and sep >= SEP_THR:\n            new_label = known_classes\n            known_classes += 1\n\n            print(f\"PROMOTED NEW CLASS {new_label}\")\n\n            model.expand_head(known_classes)\n            model.to(device)\n\n            finetune(model, memory.get(), Zc, new_label, device)\n            memory.add_batch(Zc, new_label)\n        else:\n            print(\"Blocked: cohesion/separation failed (kept)\")\n            for i in idxs:\n                new_buffer.append(novelty_buffer[i])\n\n    novelty_buffer = new_buffer\n    detector.update(memory.get())\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T04:55:12.646461Z","iopub.execute_input":"2026-02-05T04:55:12.647175Z","iopub.status.idle":"2026-02-05T04:57:09.039346Z","shell.execute_reply.started":"2026-02-05T04:55:12.647145Z","shell.execute_reply":"2026-02-05T04:57:09.038679Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/hdbscan/robust_single_linkage_.py:175: SyntaxWarning: invalid escape sequence '\\{'\n  $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 191MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\n================ TASK 0 ================\nâœ… [INFO] Task0 training complete\n\n================ TASK 1 ================\n[DEBUG] Task 1 novelty buffer size = 120\n[DEBUG] Skipping Stage II\n\n================ TASK 2 ================\n[DEBUG] Task 2 novelty buffer size = 240\n[DEBUG] Skipping Stage II\n\n================ TASK 3 ================\n[DEBUG] Task 3 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n\n================ TASK 4 ================\n[DEBUG] Task 4 novelty buffer size = 300\n[DEBUG] Entering Stage II clustering\nNovelty buffer label distribution: Counter({3: 50, 5: 49, 7: 43, 4: 36, 8: 34, 6: 33, 9: 27, 2: 26, 1: 2})\n[DEBUG] Clusters found: {np.int64(0), np.int64(1), np.int64(-1)}\nðŸ“Š [CLUSTER 0] size=53 | Scoh=0.013 | Sep=0.736 | semantic_label=3 | purity=0.21\nBlocked (kept)\nðŸ“Š [CLUSTER 1] size=48 | Scoh=0.026 | Sep=0.831 | semantic_label=3 | purity=0.21\nBlocked (kept)\nNoise cluster kept\n\n================ TASK 5 ================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[DEBUG] Task 5 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n\n================ TASK 6 ================\n[DEBUG] Task 6 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n\n================ TASK 7 ================\n[DEBUG] Task 7 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n\n================ TASK 8 ================\n[DEBUG] Task 8 novelty buffer size = 300\n[DEBUG] Entering Stage II clustering\nNovelty buffer label distribution: Counter({5: 44, 3: 40, 6: 40, 7: 37, 8: 37, 4: 36, 9: 36, 2: 27, 0: 2, 1: 1})\n[DEBUG] Clusters found: {np.int64(0), np.int64(1), np.int64(-1)}\nðŸ“Š [CLUSTER 0] size=48 | Scoh=0.024 | Sep=0.834 | semantic_label=3 | purity=0.19\nBlocked (kept)\nðŸ“Š [CLUSTER 1] size=24 | Scoh=0.011 | Sep=0.759 | semantic_label=3 | purity=0.21\nBlocked (kept)\nNoise cluster kept\n\n================ TASK 9 ================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[DEBUG] Task 9 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n\n================ TASK 10 ================\n[DEBUG] Task 10 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n\n================ TASK 11 ================\n[DEBUG] Task 11 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n\n================ TASK 12 ================\n[DEBUG] Task 12 novelty buffer size = 300\n[DEBUG] Entering Stage II clustering\nNovelty buffer label distribution: Counter({5: 54, 6: 46, 7: 45, 3: 42, 4: 31, 9: 30, 8: 29, 2: 17, 1: 5, 0: 1})\n[DEBUG] Clusters found: {np.int64(0), np.int64(1), np.int64(-1)}\nðŸ“Š [CLUSTER 0] size=67 | Scoh=0.020 | Sep=0.813 | semantic_label=5 | purity=0.22\nBlocked (kept)\nðŸ“Š [CLUSTER 1] size=16 | Scoh=0.009 | Sep=0.751 | semantic_label=3 | purity=0.31\nBlocked (kept)\nNoise cluster kept\n\n================ TASK 13 ================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[DEBUG] Task 13 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n\n================ TASK 14 ================\n[DEBUG] Task 14 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n\n================ TASK 15 ================\n[DEBUG] Task 15 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n\n================ TASK 16 ================\n[DEBUG] Task 16 novelty buffer size = 300\n[DEBUG] Entering Stage II clustering\nNovelty buffer label distribution: Counter({6: 54, 5: 47, 3: 45, 7: 42, 9: 30, 4: 27, 8: 27, 2: 26, 1: 1, 0: 1})\n[DEBUG] Clusters found: {np.int64(0), np.int64(1), np.int64(-1)}\nðŸ“Š [CLUSTER 0] size=62 | Scoh=0.029 | Sep=0.845 | semantic_label=2 | purity=0.18\nBlocked (kept)\nðŸ“Š [CLUSTER 1] size=8 | Scoh=0.008 | Sep=0.734 | semantic_label=3 | purity=0.25\nBlocked (kept)\nNoise cluster kept\n\n================ TASK 17 ================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[DEBUG] Task 17 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n\n================ TASK 18 ================\n[DEBUG] Task 18 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n\n================ TASK 19 ================\n[DEBUG] Task 19 novelty buffer size = 300\n[DEBUG] Skipping Stage II\n","output_type":"stream"}],"execution_count":4}]}